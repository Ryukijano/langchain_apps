{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbf2458",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/chain.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58238466-lesson-4-chain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e",
   "metadata": {},
   "source": [
    "# Chain\n",
    "\n",
    "## Review\n",
    "\n",
    "We built a simple graph with nodes, normal edges, and conditional edges.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, let's build up to a simple chain that combines 4 [concepts](https://python.langchain.com/v0.2/docs/concepts/):\n",
    "\n",
    "* Using [chat messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as our graph state\n",
    "* Using [chat models](https://python.langchain.com/v0.2/docs/concepts/#chat-models) in graph nodes\n",
    "* [Binding tools](https://python.langchain.com/v0.2/docs/concepts/#tools) to our chat model\n",
    "* [Executing tool calls](https://python.langchain.com/v0.2/docs/concepts/#functiontool-calling) in graph nodes \n",
    "\n",
    "![Screenshot 2024-08-21 at 9.24.03 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dd607b08df5e1101_chain1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langgraph\n",
    "%pip install --quiet -U langchain_google_genai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ac2d0-c7b0-4a20-86e5-4b6ed15ec20e",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "Chat models can use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages), which capture different roles within a conversation. \n",
    "\n",
    "LangChain supports various message types, including `HumanMessage`, `AIMessage`, `SystemMessage`, and `ToolMessage`. \n",
    "\n",
    "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call. \n",
    "\n",
    "Let's create a list of messages. \n",
    "\n",
    "Each message can be supplied with a few things:\n",
    "\n",
    "* `content` - content of the message\n",
    "* `name` - optionally, a message author \n",
    "* `response_metadata` - optionally, a dict of metadata (e.g., often populated by model provider for `AIMessages`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "866b5321-a238-4a9e-af9e-f11a131b5f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "So you said you were researching ocean mammals?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "Yes, that's right.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "Great, what would you like to learn about.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "I want to learn about the best place to see Orcas in the US.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")]\n",
    "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lance\"))\n",
    "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Lance\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca48df0-b639-4ff1-a777-ffe2185d991e",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "\n",
    "[Chat models](https://python.langchain.com/v0.2/docs/concepts/#chat-models) can use a sequence of message as input and support message types, as discussed above.\n",
    "\n",
    "There are [many](https://python.langchain.com/v0.2/docs/concepts/#chat-models) to choose from! Let's work with OpenAI. \n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5af913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de1e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI (GitHub) credentials\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    raise ValueError(\"GITHUB_TOKEN is not set\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GITHUB_TOKEN\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://models.inference.ai.azure.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2652d5ec-7602-4220-bc6e-b90783ab287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# Set Google API Key\n",
    "_set_env(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae53d4-14f5-4bf3-a953-cc465240f5b5",
   "metadata": {},
   "source": [
    "We can load a chat model and invoke it with out list of messages.\n",
    "\n",
    "We can see that the result is an `AIMessage` with specific `response_metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3712d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the Gemini model\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\",  # Specify the Gemini model variant\n",
    "    temperature=0.6  # Set temperature to control randomness\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b99ad4-5753-49d3-a916-a9e949722c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm1 = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "result1 = llm1.invoke(messages)\n",
    "type(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d60338-c892-4d04-a83f-878de4a76a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's a great question!  Orcas, also known as killer whales, are truly fascinating creatures.  \\n\\nTo help me recommend the best place for you, could you tell me a little more about what you're looking for? For example:\\n\\n* **What time of year are you planning to travel?**  Orca sightings vary seasonally depending on their migration patterns.\\n* **Do you prefer a land-based or water-based experience?**  You can see orcas from whale watching tours, kayaking trips, or even from the shore.\\n* **What kind of experience are you hoping for?** Do you want to see orcas in a wild, natural setting, or are you interested in a more interactive experience?\\n* **What is your budget?**  Whale watching tours can range in price depending on the location and type of experience.\\n\\nOnce I have a better understanding of your preferences, I can give you some specific recommendations. \\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-1ed913ad-fc3e-4e12-81b4-7f569de0a45d-0', usage_metadata={'input_tokens': 46, 'output_tokens': 197, 'total_tokens': 243})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3a29654-6b8e-4eda-9cec-22fabb9b8620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
       " 'finish_reason': 'STOP',\n",
       " 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n",
       "   'probability': 'NEGLIGIBLE',\n",
       "   'blocked': False},\n",
       "  {'category': 'HARM_CATEGORY_HATE_SPEECH',\n",
       "   'probability': 'NEGLIGIBLE',\n",
       "   'blocked': False},\n",
       "  {'category': 'HARM_CATEGORY_HARASSMENT',\n",
       "   'probability': 'NEGLIGIBLE',\n",
       "   'blocked': False},\n",
       "  {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',\n",
       "   'probability': 'NEGLIGIBLE',\n",
       "   'blocked': False}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1feea435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='One of the best places to see orcas in the US is the San Juan Islands in Washington State. Here are a few things to know:\\n\\n1. **Location**: The San Juan Islands are located in the Pacific Northwest, accessible by ferry from Anacortes, Washington.\\n\\n2. **Orca Pods**: The area is home to both resident and transient orca pods. The Southern Resident Killer Whales are the most well-known, though their population is considered endangered.\\n\\n3. **Best Time to Visit**: The prime time for orca sightings is typically from late spring to early fall, with the peak being around June to September.\\n\\n4. **Tours**: There are numerous whale-watching tours departing from Friday Harbor and other parts of the islands, offering guided experiences to see these magnificent creatures.\\n\\n5. **Conservation**: It’s essential to choose responsible tour operators who follow guidelines to minimize disturbance to the whales.\\n\\nLet me know if you need more information!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 197, 'prompt_tokens': 67, 'total_tokens': 264, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_67802d9a6d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba2d940c-7a63-4ec7-85bc-59caf04b25d6-0', usage_metadata={'input_tokens': 67, 'output_tokens': 197, 'total_tokens': 264})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the OpenAI model\n",
    "llm2 = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Invoke the model with the same messages\n",
    "result2 = llm2.invoke(messages)\n",
    "result2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4718bd5c-5314-4405-a164-f1fe912ae306",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Tools are useful whenever you want a model to interact with external systems.\n",
    "\n",
    "External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. \n",
    "\n",
    "When we bind an API, for example, as a tool we given the model awareness of the required input schema.\n",
    "\n",
    "The model will choose to call a tool based upon the natural language input from the user. \n",
    "\n",
    "And, it will return an output that adheres to the tool's schema. \n",
    "\n",
    "[Many LLM providers support tool calling](https://python.langchain.com/v0.1/docs/integrations/chat/) and [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple. \n",
    " \n",
    "You can simply pass any Python `function` into `ChatModel.bind_tools(function)`.\n",
    "\n",
    "![Screenshot 2024-08-19 at 7.46.28 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dc1c17a7a57f9960_chain2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a942b1",
   "metadata": {},
   "source": [
    "Let's showcase a simple example of tool calling!\n",
    " \n",
    "The `multiply` function is our tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "928faf56-1a1a-4c5f-b97d-bd64d8e166d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm2.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9dba",
   "metadata": {},
   "source": [
    "If we pass an input - e.g., `\"What is 2 multiplied by 3\"` - we see a tool call returned. \n",
    "\n",
    "The tool call has specific arguments that match the input schema of our function along with the name of the function to call.\n",
    "\n",
    "```\n",
    "{'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9edbe13e-cc72-4685-ac97-2ebb4ceb2544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_NG7yFh4ckmeIL1afB335pPEP', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_67802d9a6d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-dc926ee3-3b85-44ac-af4f-7ad29b798c79-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_NG7yFh4ckmeIL1afB335pPEP', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d09cd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_NHeP7mR0DTrEURAujFE51LZb', 'type': 'tool_call'}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model\n",
    "response = llm_with_tools.invoke([HumanMessage(content=\"What is 2 multiplied by 3?\")])\n",
    "\n",
    "# Access tool calls\n",
    "tool_call = response.tool_calls[0]\n",
    "print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78178cb-fa43-45b5-be5e-5a22bda5a5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'call_NG7yFh4ckmeIL1afB335pPEP',\n",
       "  'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'},\n",
       "  'type': 'function'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.additional_kwargs['tool_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f389fe70",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'parent_run_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Output: 6\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:179\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     emit_warning()\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_core/tools/base.py:809\u001b[0m, in \u001b[0;36mBaseTool.__call__\u001b[0;34m(self, tool_input, callbacks)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.47\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tool_input: \u001b[38;5;28mstr\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    808\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make tool callable.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_core/tools/base.py:616\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    580\u001b[0m     tool_input: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the tool.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m        ToolException: If an error occurs during tool execution.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     callback_manager \u001b[38;5;241m=\u001b[39m \u001b[43mCallbackManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m     run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_tool_start(\n\u001b[1;32m    627\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription},\n\u001b[1;32m    628\u001b[0m         tool_input \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_input, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(tool_input),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    638\u001b[0m     )\n\u001b[1;32m    640\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_core/callbacks/manager.py:1563\u001b[0m, in \u001b[0;36mCallbackManager.configure\u001b[0;34m(cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure\u001b[39m(\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1541\u001b[0m     local_metadata: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CallbackManager:\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Configure the callback manager.\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m \n\u001b[1;32m   1545\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;124;03m        CallbackManager: The configured callback manager.\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_configure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43minheritable_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43minheritable_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43minheritable_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/langchain_core/callbacks/manager.py:2253\u001b[0m, in \u001b[0;36m_configure\u001b[0;34m(callback_manager_cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     callback_manager \u001b[38;5;241m=\u001b[39m callback_manager_cls(\n\u001b[1;32m   2248\u001b[0m         handlers\u001b[38;5;241m=\u001b[39minheritable_callbacks_\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[1;32m   2249\u001b[0m         inheritable_handlers\u001b[38;5;241m=\u001b[39minheritable_callbacks_\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[1;32m   2250\u001b[0m         parent_run_id\u001b[38;5;241m=\u001b[39mparent_run_id,\n\u001b[1;32m   2251\u001b[0m     )\n\u001b[1;32m   2252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2253\u001b[0m     parent_run_id_ \u001b[38;5;241m=\u001b[39m \u001b[43minheritable_callbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_run_id\u001b[49m\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# Break ties between the external tracing context and inherited context\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parent_run_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   2256\u001b[0m         parent_run_id_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2257\u001b[0m         \u001b[38;5;66;03m# If the LC parent has already been reflected\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2260\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (run_tree \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(parent_run_id_) \u001b[38;5;129;01min\u001b[39;00m run_tree\u001b[38;5;241m.\u001b[39mdotted_order)\n\u001b[1;32m   2261\u001b[0m     ):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'parent_run_id'"
     ]
    }
   ],
   "source": [
    "result = multiply(tool_call['args']['a'], tool_call['args']['b'])\n",
    "print(f\"Result: {result}\")  # Output: 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c10f9a-2372-486b-9305-55b7c41ecd6e",
   "metadata": {},
   "source": [
    "## Using messages as state\n",
    "\n",
    "With these foundations in place, we can now use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages) in our graph state.\n",
    "\n",
    "Let's define our state, `MessagesState`, as a `TypedDict` with a single key: `messages`.\n",
    "\n",
    "`messages` is simply a list of messages, as we defined above (e.g., `HumanMessage`, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3699dd5c-398c-43c7-b496-fd87e55e11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: list[AnyMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cba3e-ebba-4b91-a539-1cbc28b4a40e",
   "metadata": {},
   "source": [
    "## Reducers\n",
    "\n",
    "Now, we have a minor problem! \n",
    "\n",
    "As we discussed, each node will return a new value for our state key `messages`.\n",
    "\n",
    "But, this new value will [will override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior `messages` value.\n",
    " \n",
    "As our graph runs, we want to **append** messages to to our `messages` state key.\n",
    " \n",
    "We can use [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) address this.\n",
    "\n",
    "Reducers allow us to specify how state updates are performed.\n",
    "\n",
    "If no reducer function is specified, then it is assumed that updates to the key should *override it* as we saw before.\n",
    " \n",
    "But, to append messages, we can use the pre-built `add_messages` reducer.\n",
    "\n",
    "This ensures that any messages are appended to the existing list of messages.\n",
    "\n",
    "We annotate simply need to annotate our `messages` key with the `add_messages` reducer function as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b33eb72-3197-4870-b9a3-0da8056c40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663e574-ba15-46be-a37c-48c8052d693b",
   "metadata": {},
   "source": [
    "Since having a list of messages in graph state is so common, LangGraph has a pre-built [`MessagesState`](https://langchain-ai.github.io/langgraph/concepts/low_level/#messagesstate)! \n",
    "\n",
    "`MessagesState` is defined: \n",
    "\n",
    "* With a pre-build single `messages` key\n",
    "* This is a list of `AnyMessage` objects \n",
    "* It uses the `add_messages` reducer\n",
    "\n",
    "We'll usually use `MessagesState` because it is less verbose than defining a custom `TypedDict`, as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ab516ee-eab1-4856-8210-99f1fe499672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class MessagesState(MessagesState):\n",
    "    # Add any keys needed beyond messages, which is pre-built \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0fff7-60a2-4582-8f12-3a3ab6633d6c",
   "metadata": {},
   "source": [
    "To go a bit deeper, we can see how the `add_messages` reducer works in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23ffea76-16a5-4053-a1bc-91e0101d91dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='105754ed-cc8d-4894-bbda-4d8a79906846'),\n",
       " HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Lance', id='3036a9b5-9891-47fc-b9ba-6ba556aeb3fa'),\n",
       " AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='fe964fd6-101c-4186-9d31-81a57af00f18')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial state\n",
    "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n",
    "                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")\n",
    "                   ]\n",
    "\n",
    "# New message to add\n",
    "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n",
    "\n",
    "# Test\n",
    "add_messages(initial_messages , new_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485adccc-f262-49dd-af4f-a30e9b6a48e2",
   "metadata": {},
   "source": [
    "## Our graph\n",
    "\n",
    "Now, lets use `MessagesState` with a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5306639-7e6a-44be-8471-8d2631701cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCADqAKEDASIAAhEBAxEB/8QAHQABAAIDAQEBAQAAAAAAAAAAAAUGAwQHCAIBCf/EAE8QAAEDAwEDBggKBwUGBwAAAAEAAgMEBREGBxIhExQVMZTTCBYiQVFUVmEXIzI0VXFydLHRNkKBkpWhsiQmN3OTCSVDYpHSMzVEUlNlpP/EABoBAQEAAwEBAAAAAAAAAAAAAAABAgMGBAX/xAAyEQEAAQIBCQYGAwEBAAAAAAAAAQIRAwQSFCExUXGR0UFSYWKSoQUTIzOxwVPh8DKB/9oADAMBAAIRAxEAPwD+qaIiAiIgItC9XiGx0LqmVkkziQyKnhAMk0h+SxgJAyT6SAOJJABIhhpOXUDeW1LLzrfH/lcMhFHFx6jwBlPmLn8D5mtzhbaaImM6qbR/ti2TE+obVSyFk1zo4njra+oY0/8AQlY/Gqy/TFB2ln5rHDo6wU7d2Kx22NvXhlJGB+CyeKtl+h6DszPyWf0fH2XUeNVl+mKDtLPzTxqsv0xQdpZ+aeKtl+h6DszPyTxVsv0PQdmZ+SfR8fY1HjVZfpig7Sz808arL9MUHaWfmnirZfoeg7Mz8k8VbL9D0HZmfkn0fH2NR41WX6YoO0s/NfUepLRM8NjutFI49QbUMJ/FfPirZfoeg7Mz8l8S6RsUzCySy26Rh62upIyD/JPo+PsmpLAggEHIK/VWDomK0/HacmNkmBLuax8aOX/lfF1NHvj3Xe8jgZSxXoXiCUSQOo66nfydTSPOTE/3H9ZpHFrvOD5jkDGqiLZ1E3j3LbkmiItKCIiAiIgIiICIiCsDF32gvbJh0NmpGSRtOeE8xeC70ZEbMA+iR3pVnVYto5ntBvcT8jntHTVMZxwduGSN4z6R8X+8FZ16MbbTEbLR+Lz73WRERedHL4PCR0JdaDUNRZbpNeZLLR1FbKymoKosmZC7ceYpOSLZQHkNJjL8ZWlobwltL6l2Q27Xd1NZZaaWCm53DJbas8nUTMa4RQ5hDqgZdgPia5rvMVzbZZbr7TapvGk9IWTVdn2dVVorzLbNW2808VqrnvxHHRTO8qSN+/ISwF7W4BDhnC0bTqTWVN4Oug9NUGnta6dqdPvtlo1S6jtMja4UbIXxzOoTunlfjIo8vh3iGPyOPUHcovCA2fy6AqtajUcLNNUdUyhqqySCZhpp3SMjEcsZYJIzvSMzvNGA4E4HFVLVvhX6X07eNGQU1Ldq63X6tqaWWrbZbgHwsip3S78cQpy6beduAbo+SXOGQ0kcRl0Jea3Qu1u30ulNXOo7rqnT1yoIb9BPU1dXSiekbLI5zy9ziOQkc5rjvMZu74b1Lu+3+C42vVey7V1LZbnfbfp69TyXCns9K6qqmRTUU8AkbE3ynhr3tzugkA5wg7FTzsqqeKaPe5ORoe3faWnBGRkEAg+48VkWra69t1tlHWtgnpm1MLJhDVRGKWMOaDuvYeLXDOCDxByFtICrF5xadZWOuZutFyL7ZUdeX4Y+aEn7JZKB/mlWdVjVTeeag0rRtyXitfWPwMgRxwvBOfN5ckY/avRgf9THhP4lYWdERedBERAREQEREBERBDais81caWuoDGy7UDi+nMpIZI1ww+J5GSGuHnwd1wa7Dt3BxxVlo1vaq62VlKyZksToK+0XCNpe1jgWuZLGcgtIyMjLXDiC4EFTqir1pi2agMbq2m3poxiOphkdDPGPOGysIe3zdRHUt1NVNURTX2dq8VMZ4NuyiNwc3ZvpZrgcgi0wAg/ur6h8HHZVTTRyxbOdLxyxuDmPbaYAWkcQQd1Tx0PI0nktS36Fp/V50x+P2vY4/wA08Saj2qv3+tD3Sy+Xh9/2ktG9aEVX8Saj2qv3+tD3Sxz6MqY4JHjVV+y1pIzND6P8pPl4ff8AaS0b1sRcv2WWu66w2Y6Qv1x1TeBcLpZ6OuqRTywiPlZIWPfu/FnycuOOJ4edWjxJqPaq/f60PdJ8vD7/ALSWjeibtsB2aX651VxuWgdOV9wqpHTT1VRa4XySvccuc5xbkkniSVrP8G7ZTIcv2caXcQAMm0wHgBgD5PoCn/Emo9qr9/rQ90niRK4bsmp79I09Y5xG3+bYwf5p8vD7/tJaN7Zgj0/s4sFJb6KlpbPbYcx0luoYQwEkl25FEweUSSTutGeJX7YbZUyV9RerlEIbhUsEMdMHB3NYGklrCQSC8k7zy3hnDQXBgcc1n0la7JUOqaeB8tY4EOq6uZ885B6xvvJcB7gQPcphSaqaYmmjt7TgIiLQgiIgIiICIiAiIgIiICIiAsVX81m+w78FlWGr+azfYd+CCk7BC07C9nJYSWnTduwSMEjmsfvP4n61fFRNgmfgM2dZLSfFy3ZLAAPmsfVu8MfVwV7QEREBERAREQEREBERAREQEREBERAREQFhq/ms32Hfgsyw1fzWb7DvwQUjYCANhGzgBzXgabtvlMGAf7LHxHAcP2K+qhbAMfARs33SS3xatuCW7v8A6WPzeZX1AREQEREBERAREQEREBF+OcGNLnENaBkkngAqUdYXu7AVFltlCba/jDUXCpfHJM3zPEbYzutPWMnJHWAt2HhVYt83otrrsipHTusPULH2ubu06d1h6hY+1zd2t2i1745wWXdFSOndYeoWPtc3dp07rD1Cx9rm7tNFr3xzgsu6KkdO6w9Qsfa5u7Tp3WHqFj7XN3aaLXvjnBZd1xjwpdvlf4O2hqXUsWk3aotktRzSrdHXc2dSlw+LcRyb95pIcCeGDu9e9wuHTusPULH2ubu1XNo1gvu0/Q170rebZZJLbdaZ1NLiqlLmZ4te3MXymuDXD3tCaLXvjnBZQ/AV29TbadmDaAaXlsdDpWjobRFXPqhK2ueyEteWtEbAzdDGHAz/AOIOrHH0suGbDtnV52E7NrXo+zUdlqIKQOfNVyVErX1Mzjl8jgI+s8B7gAPMr507rD1Cx9rm7tNFr3xzgsu6KkdO6w9Qsfa5u7Tp3WHqFj7XN3aaLXvjnBZd0VI6d1h6hY+1zd2nTusPULH2ubu00WvfHOCy7oqR07rD1Cx9rm7tOndYeoWPtc3dpote+OcFl3RUpup9S0Xx1baKCppm8ZG2+qkdMG+cta6MB56+GR1cMngrdQ1sFyooKumkE1NPG2WKRvU5rhkEfWCtOJg14eurqWZ0RFpRF6oJbpm7kHBFHMQR9gqvaZAGm7UAAAKSLAH2ArDqr9GLx9zm/oKr2mv0ctX3SL+gL6OD9meP6XsSSItGz3y36houeWutguFJyskPL00gezfjeWPbkcMtc1zT6CCskbyIsVXVRUNLNUzu3IYWOke7BOGgZJwOPUEGVFG6Z1HbtYaett8tFRzu13GnZVUs+45nKRPaHNduuAcMgjgQCpJARFD6i1dadJutTbrV81N0ro7bRjk3v5WoeHFjPJBxkMdxOBw4lQTCKvR7QLBLNqeJlfvSaaIF1byMn9mzCJx+r5fxbmu8je68dfBStlvFJqGz0N0oJeXoK6COpp5d1zd+N7Q5rsOAIyCDggH0oNxERUEUPctXWm0ajs1iq6vkrreGzuoafk3u5YQta6XygC1uA5p8ojOeGU0zq606xpayps9XzyGjrZ7fO7k3s3J4XmOVmHAZ3XNIyOBxwJCgmERFQWDZac7PbF7qYAe4ZOFnWDZb/h7Yvu4/EqYv2J4x+JXsWpERfNRF6q/Ri8fc5v6Cq9pr9HLV90i/oCsOqv0YvH3Ob+gqvaa/Ry1fdIv6Avo4P2Z4/pexvVEIqYJInOexsjS0ujcWuGRjII4g+8Lx1okVmzDwVr5f9P3m6012q73UWp1VW3GaqhoGPvL6d07IpHFjHhjy4uABc7ynZ4r2SqJTbDdD0ldqGpjsMf8AeBkrLnTPnlfTVAlIdIeQLzG1zi0Eua0EnzqTF0cQ2kam1B4PF81Hb9O6hvWpIpNFV17bBfq11fJRVVPJGxlQHPyQxwldlnySY+AHFSFypblsx1RoagpdY33VFNq+1XOK5w3ivdVskdFRGdtVCHcIRvDdLWYbiRoxkBdj0dsV0XoMXLoeyMa64wimq5K2olrJJYQCBEXzPe7k8E+QDu8epYtE7DND7O7o+42GxNpa10BpWyzVM1QYYScmKISvcImZA8hm6OA4cFM2Ro+DQQfB62b4Of7v0XV/ktUVt1vF0qNVbOdGUV5q9OW/U9xqIq+50EgiqBHDTPmbBHJjyHSOAG8PKwDjrUxS7N7xoOihtOzep0/pqwNL5nUNyttTXFsr3EuMZFXGGM6sMAwDnHXhbFVs3qdfaeqbRtL6D1NSmaOemFsoJ6EwPbnDw41Ejw8Z4OY5pAyOOVddrChbTLGNI2nTWh7VeNbX293u4zT0MbNSPppjHFDmUTVpDpGwMBa7A3nlxAGRkLnlo1BfrxorQNHqSrkrLjZNrAtHL1FTzmUxxGcMa+bdZyrmh27vlrS7AJAJXeHeDzoF9jp7UbLMKenrHV8U4uVUKpk7mBjnioEvK5cwBp8vBAAPUsg8H3Z83Stdppmm4YrHW1bK+WjinlY1tS0NDZoyHgxvwxvlMLSTknJJJxzZuOXVcEk9T4T/ACNdW26oglp6mKpt1U+mnjfHaYJGFsjCHN8pgzg8RkHgStXSNJddpOvNNWa5at1LR25+za03OWO2XWWmdLVvllaZ3Pad4vx1nPlYG9vYAHcqvZVpWtvl8vEtpb0lfLf0Xcp45pGc6p93d3XhrgC4N4B+N4DgCAtix7OtPabu1Lc7db+b11NaYbHFLy0jt2jicXRxYc4g4Lid4jeOeJKuaPLds2lXTUmmtlMmu9V3+x6auFjruWu1iklhnrbpDM2ONsskLS8ZibI8NGA92c5xhegPByp9QU+xXS41Sa436SGSapdcnvdUu35XvaZN8ktcWOaS39UndAAGBT9pXg8Gqt2lqDRdjsvMLLDUwMhuN6ulBNGyV7XkMnpnlzmlzSS14dk7uC3CmNDaB2p6L0jbbRHrSwVr4GyGSS7WurrpGl0r3iNsxrGOcxjXNY0vBdhuSeOBIiYnWP3aQceEVsb98N8H/wCeFclhuNyodlNdFartW2Wet2uT2+Wqt8vJzNimu7mPAPEcWuPAgj0grv7dnMuq2Wuo18LRfbtZ7g2vtdXaaSeg5q9oGOueRxJIORvbrhgFpxx+37FNFvrrhV9CMjmr7rT3up5KolYyStgfvxT7jXhodvcTgAPPyg5W0yODXjS90o73tst1NrvWUVJpOz011s7TfZ3ugqJKaaRxe9xL5Wb0DcRyFzQHO4ceHpDZ7eqjUmgNM3esINXX2ymqpi0YBe+JrnYHm4krHUbOtPVVZqeqlt+9PqWljorq/lpBzmFjHxsbjewzDZHjLMHj15AUxZ7RSWC0UNroIuQoaKBlNTxbxduRsaGtbkkk4AAySSrEWG4sGy3/AA9sX3cfiVnWDZb/AIe2L7uPxKyxfsTxj8SvYtSIi+aiL1V+jF4+5zf0FV7TX6OWr7pF/QFcaiCOqgkhlbvxSNLHNPnBGCFQ4aW/6Zp4bc2yTXynp2NihrKOoha57AMN5Rsr2YfgccEg9fDO6PoZPMTRNF7Te+ubfllGuLJ1FCdLX72MuvaqLv06Wv3sZde1UXfrfmeaPVHUsm0UJ0tfvYy69qou/Tpa/exl17VRd+mZ5o9UdSybRQnS1+9jLr2qi79Olr97GXXtVF36Znmj1R1LJtFCdLX72MuvaqLv1+PvN9jY5ztG3UNaMk85o+/TM80eqOpZOIqpp7Wtw1XYLZe7XpO61NsuVLFWUs/L0jOUikYHsduumBGWuBwQCPOFIdLX72MuvaqLv0zPNHqjqWTaKE6Wv3sZde1UXfp0tfvYy69qou/TM80eqOpZNooTpa/exl17VRd+nS1+9jLr2qi79MzzR6o6lk2ihOlr97GXXtVF36dLX72MuvaqLv0zPNHqjqWTawbLf8PbF93H4lRrajUlw+Ig07LbJH8OdXGpgdHF/wA27FI9ziOJDeGSMFzc5FusdohsFmorbTue6GlhbC10hy52BjJPnJ6z7ytOPMU4eZeJmZidUxOy+7ibIbyIi+cxEREBERAREQEREBYqv5rN9h34LKsNX81m+w78EFM2Ejd2IbPRjdxp23DG7u4/s0fmwMf9B9QV5VE2CM5PYXs6YGuaG6ctw3XN3SP7LHwIycfVlXtAREQEREBERAREQEREBERAREQEREBERAWGr+azfYd+CzLDV/NZvsO/BBSNgJB2EbOC05adN23BLQ3I5rH5h1fUr6qLsGDxsN2diQyGTxct28ZRh5PNo87w9PpV6QEREBERAREQEREBERAREQEREBERARRN31bY9PyNjud5oLdI7GGVVSyNxz1YDiCVF/Cro32qs/bY/wA1upwMWuL00zMcJW0rUq/q/W+ndHUrOn7/AGux85Y8QdJVkdPyu6Bvbu+4b2N5ucdWR6Vq/Cro32qs/bY/zXD/AAx7BovbzsRu1ppNR2aXUFv/AN42l3PY94zsBzGDn/iNLm46slpPUstGx+5PKVzZ3Ok+DXquw6h2L6Jo7Pebfc6i3aftsVXBRVUcr6Z3NmgNka1ziw5Y4YJ/VPXgrqK8bf7P/SmmNiWyGauvl9tlBqjUcraqtpp6tjJKeJm82CJzSeDgHPcfRymD1L0/8Kujfaqz9tj/ADTRsfuTykzZ3LUiqvwq6N9qrP22P81v2rW+nb7PyFtv1tr5/wD4qarjkf8Aug5UnAxaYvVRMRwlLSm0RFoQREQEREBERAREQEREBcZ2jbTKq51s9osdTJSUUDnRVNdA7dkmeOBZG4cWtacguHEkYGAMu6DtIvk2ndC3qvpn8nVMpyyB/wD7JXkMY79jnA/sXnqmp2UtPHDGMMjaGge4Lpvg+R0Yt8fEi9tUcV2Rd8wUcFM5zoomMe45c8DynH0k9ZPvKzIi7JgIq9q/W9HpAUUUlLW3O4Vz3MpbdbohJPNujLyASGhrRjLnEAZHHJCr0226yU9tgqZLfd21cl0bZ32zmg53DVOidKxr2b2MOa0Yc0keUDnGSNNWNh0zaqUdCRUWLbFY2WK+XK4QV9nksszKest9bCOcskeGmJrWsc4PL99u7uk5yorSG0e56q2sVlrkt9zslsiscVULfdaaOKXljO9vKAtLiQW4GN7AIPAHKx+fh3iInb/v0rp6xVFJBVtDZoY5QDkB7QcFZUXoFu0NtKqtJTx0t1qpauxHyTLUPL5KTj8reJyYx5wc7o4jgMLu4ORkcQvLLmh7S1wDmkYIPUV27YxdpLnoKkimeZJaGSSiLz1lrHEMz79zc/auS+M5HRREZRhxbXaevVltheURFygIiICIiAiIgIiIKptUtk132e3yCna6SdtPy8cbPlPdGRIGj3ksx+1cCilZPEySNwcx4DmuHnB6l6oXBNoWgJdF1M1dRRGTT8jnSEsb8yJOS1wH/D68O6m9RwACep+C5VRRfJ65tebx06G2HN71r7TGm6zml31HabXV7ofyFbXRQybp6juucDjgeK0Phd0L7aae/isH/erNyVPVBsu5FMHDg/AdkfWnMab1eL9wLqpjEvqmOX9sXH9o9hoNpt305qewUlm2h0NndUUlZaW1cEjJGytYd5j3EsEjC1pw4jId1hZTs+qJLfo+W1aKo9KOptSxXCtoaSWDyIGQzMEryzDXO8to3W7xGfPxXX44mQgiNjWA8cNGF9LRo1M1TVVtnh4f+9m9HE9Y7NNR3bUOsbrQUkTpulLPdrXHPM1sdY6lYN+NxBJZkgjLgOOPNxUja7jcqLaXV6x1bbINF2Z1kitrJblc6ZwMwqHv3SWvwODuHpx6eA62vmSNkrd17Wvb6HDIV0aIqzqZnbfwveZ/aqr8LmhTn++mnuH/ANrB/wB62rXtI0le6+Kht2qbLX1sxIjpqW4QySPIBJw1riTwBPD0Kc5lT+rxfuBfraaCI74ijYRx3g0DC3RGJfXMcv7RlXaNiFA+l0FDUvBbz+omrGA+djnYYfqLGtd+1cz0XomfaBOMb8Vh4iorW8OVGcGOI+cniC4cG+nK9Dwwx00McMTGxRRtDGMYMNaAMAAeYLmvjWVUTTGT0zeb3nw8GcaofaIi5EEREBERAREQEREBERBUbpsm0nd6h1RNZo4JnO3nPopJKYuPpdyTm5P1qP8AgN0j6rX/AMWq+9V+ReynLMppi1OJVEcZW8qD8BukfVa/+LVfep8BukfVa/8Ai1X3qvyLLTsq/lq5yXlQfgN0j6rX/wAWq+9T4DdI+q1/8Wq+9V+RNOyr+WrnJeVB+A3SPqtf/FqvvVt0Gx3R9vmbKLOKp4II5/US1TQR1Hdlc4fyVzRYzluVVRacWrnJeXyxjY2NYxoa1owGgYAHoX0iLxoIiICIiAiIg//Z",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "    \n",
    "# Node\n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_edge(\"tool_calling_llm\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8909771-7786-47d6-a53d-6bbc3b365737",
   "metadata": {},
   "source": [
    "If we pass in `Hello!`, the LLM responds without any tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "983e2487-c0a5-40a2-afbc-aa53ff49fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588688b-efd9-4dbc-abf2-7903e3ef89ba",
   "metadata": {},
   "source": [
    "The LLM chooses to use a tool when it determines that the input or task requires the functionality provided by that tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fe8b042-ecc8-426f-995e-cc1bbaf7cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_0qXjsJvjdhpEK5y7N7bdhmoJ)\n",
      " Call ID: call_0qXjsJvjdhpEK5y7N7bdhmoJ\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fbae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
