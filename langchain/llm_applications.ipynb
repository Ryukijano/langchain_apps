{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "## Installation\n",
    "\n",
    "Ensure you have the necessary libraries installed. You've already initiated this step, but for completeness, here's how to install the required packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U langchain_google_genai langchain_core langchain_community tavily-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# Set Google API Key\n",
    "_set_env(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Set LangSmith API Key (if using LangSmith for tracing)\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Set Tavily API Key (if using TavilySearchResults)\n",
    "_set_env(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The langserve[all] installation includes all optional dependencies required for serving applications.\n",
    "### 🌍 Environment Variables\n",
    "\n",
    "Properly setting environment variables is crucial for authenticating and utilizing Google's Gemini models and other services.\n",
    "\n",
    "In your Python environment or Jupyter Notebook, set the following environment variables:\n",
    "\n",
    "- 🔑 **GOOGLE_API_KEY**\n",
    "- 🔑 **LANGCHAIN_API_KEY** (if using LangSmith for tracing)\n",
    "- 🔑 **TAVILY_API_KEY** (if using TavilySearchResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Language Models\n",
    "### 🌟 Utilizing Google's Gemini Models\n",
    "\n",
    "Instead of using OpenAI's models, we'll utilize Google's Gemini models through `ChatGoogleGenerativeAI`.\n",
    "\n",
    "#### 📥 Importing and Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the Gemini model\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\",  # Specify the Gemini model variant\n",
    "    temperature=0.6  # Set temperature to control randomness\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "### 🔧 Model Configuration\n",
    "\n",
    "- **🛠️ Model**: Specifies which Gemini model to use. Options include:\n",
    "    - `gemini-1.5-flash-latest`\n",
    "    - `gemini-1.5-pro-latest`\n",
    "    \n",
    "- **🌡️ Temperature**: Controls the randomness of the output. Lower values make the output more deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Direct Model Call\n",
    "### 💬 Interacting with the Model\n",
    "\n",
    "Let's interact with the model directly by sending a list of messages. 📩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao, come stai? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian.\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\")\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the response\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Parsers\n",
    "### 📋 Extracting Textual Responses\n",
    "\n",
    "The model's response comes with additional metadata. Often, you may only need the textual response. OutputParsers help extract the desired information.\n",
    "\n",
    "#### 🛠️ Using `StrOutputParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao, come stai? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Invoke the model\n",
    "result = model.invoke(messages)\n",
    "\n",
    "# Parse the response\n",
    "parsed_response = parser.invoke(result)\n",
    "\n",
    "print(parsed_response)  # Output: 'Ciao, come stai?'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining Model with Parser Using LCEL\n",
    "### 🔗 Chaining Components with LangChain\n",
    "\n",
    "LangChain allows chaining components using the `|` (pipe) operator. 🚀\n",
    "\n",
    "This feature enables seamless integration and interaction between different components, making your workflow more efficient and intuitive. 🌟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao, come stai? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chain the model with the parser\n",
    "chain = model | parser\n",
    "\n",
    "# Invoke the chain\n",
    "parsed_response = chain.invoke(messages)\n",
    "\n",
    "print(parsed_response)  # Output: 'Ciao, come stai?'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits:\n",
    "### 🚀 Simplified Workflow\n",
    "\n",
    "- **Streamlines Processing**: Automatically passes the model's output to the parser. 🔄\n",
    "\n",
    "### 🔍 Enhanced Tracing\n",
    "\n",
    "- **Traceable Components**: Each component in the chain is traceable via LangSmith. 🛠️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates\n",
    "### 📝 PromptTemplates: Structuring Your Input\n",
    "\n",
    "PromptTemplates help in structuring the input to the language model, making it dynamic and reusable. ✨\n",
    "\n",
    "#### 🎨 Creating a ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the system template with placeholders\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "# Create the ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', '{text}')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Prompt Template```markdown\n",
    "Using the Prompt Template\n",
    "### 📝 Utilizing the ChatPromptTemplate\n",
    "\n",
    "The `ChatPromptTemplate` allows you to create structured and dynamic prompts for the language model. This ensures consistency and reusability in your interactions with the model.\n",
    "\n",
    "#### 🎨 Creating a ChatPromptTemplate\n",
    "\n",
    "In the previous cell, we defined a `ChatPromptTemplate` with placeholders for `language` and `text`. This template can be used to generate prompts dynamically based on the input values provided.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### 📝 Utilizing the ChatPromptTemplate\n",
    "\n",
    "In this section, we demonstrate how to use the `ChatPromptTemplate` to generate dynamic prompts for the language model. This ensures consistency and reusability in your interactions with the model. 🌟\n",
    "\n",
    "#### 🎨 Creating Input Variables\n",
    "\n",
    "First, we define the input variables that will be used to fill in the placeholders in our prompt template:\n",
    "\n",
    "```python\n",
    "input_variables = {\n",
    "    \"language\": \"Italian\",\n",
    "    \"text\": \"Good morning!\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### 🛠️ Generating the Prompt\n",
    "\n",
    "Next, we generate the prompt by invoking the `ChatPromptTemplate` with the input variables:\n",
    "\n",
    "```python\n",
    "prompt = prompt_template.invoke(input_variables)\n",
    "```\n",
    "\n",
    "#### 🔄 Converting Prompt to Messages\n",
    "\n",
    "The generated prompt is then converted into a list of messages that can be sent to the language model:\n",
    "\n",
    "```python\n",
    "messages = prompt.to_messages()\n",
    "```\n",
    "\n",
    "#### 📩 Displaying the Messages\n",
    "\n",
    "Finally, we display the messages to see the structured input that will be sent to the model:\n",
    "\n",
    "```python\n",
    "for message in messages:\n",
    "    print(f\"{message.type}: {message.content}\")\n",
    "```\n",
    "\n",
    "This process ensures that your prompts are dynamically generated and consistently formatted, making your workflow more efficient and effective. 🚀\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: Translate the following into Italian:\n",
      "human: Good morning!\n"
     ]
    }
   ],
   "source": [
    "# Define input variables\n",
    "input_variables = {\n",
    "    \"language\": \"Italian\",\n",
    "    \"text\": \"Good morning!\"\n",
    "}\n",
    "\n",
    "# Generate the prompt\n",
    "prompt = prompt_template.invoke(input_variables)\n",
    "\n",
    "# Convert prompt to messages\n",
    "messages = prompt.to_messages()\n",
    "\n",
    "# Display messages\n",
    "for message in messages:\n",
    "    print(f\"{message.type}: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining Components with LCEL\n",
    "### 🔗 Building the Chain: Streamlined Translation Workflow\n",
    "\n",
    "By chaining the `prompt_template`, `model`, and `parser`, you can create a streamlined and efficient workflow for translation. 🌟\n",
    "\n",
    "#### 🛠️ Steps to Build the Chain:\n",
    "\n",
    "1. **Define Input Variables**: Set the language and text to be translated.\n",
    "2. **Generate the Prompt**: Use the `ChatPromptTemplate` to create a structured prompt.\n",
    "3. **Convert to Messages**: Transform the prompt into a list of messages.\n",
    "4. **Invoke the Model**: Send the messages to the model for translation.\n",
    "5. **Parse the Response**: Extract the textual response using the `StrOutputParser`.\n",
    "\n",
    "This process ensures consistency, reusability, and efficiency in your translation tasks. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buongiorno! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chain the prompt template, model, and parser\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "# Define the input for the chain\n",
    "input_data = {\n",
    "    \"language\": \"Italian\",\n",
    "    \"text\": \"Good morning!\"\n",
    "}\n",
    "\n",
    "# Invoke the chain\n",
    "translated_text = chain.invoke(input_data)\n",
    "\n",
    "print(translated_text)  # Output: 'Buongiorno!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import diskcache\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Gemini model\n",
    "model_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\",\n",
    "    temperature=0.7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message template\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an intelligent reasoning assistant.\\n\"\n",
    "    \"Maintain the following conversation history:\\n\"\n",
    "    \"{history}\\n\\n\"\n",
    "    \"Provide a thoughtful and detailed response to the user's input.\"\n",
    ")\n",
    "\n",
    "# Human message template\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "\n",
    "# Combine into a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([system_message, human_message])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get or create memory per session\n",
    "session_memories = {}\n",
    "\n",
    "def get_memory(session_id):\n",
    "    if session_id in session_memories:\n",
    "        return session_memories[session_id]\n",
    "    else:\n",
    "        session_memories[session_id] = ConversationBufferMemory(\n",
    "            memory_key=\"history\",\n",
    "            return_messages=True,\n",
    "        )\n",
    "        return session_memories[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Function to create the reasoning agent chain\n",
    "def create_reasoning_agent_chain(model_name, session_id):\n",
    "    selected_model = get_model(model_name)\n",
    "    memory = get_memory(session_id)\n",
    "    chain = (\n",
    "        prompt_template\n",
    "        .partial()\n",
    "        | selected_model\n",
    "        | parser\n",
    "    )\n",
    "    return chain, memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    if model_name.lower() == \"openai\":\n",
    "        return model_openai\n",
    "    elif model_name.lower() == \"gemini\":\n",
    "        return model_gemini\n",
    "    else:\n",
    "        raise ValueError(f\"Model '{model_name}' is not supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_agent(user_input, model_name, session_id):\n",
    "    chain, memory = create_reasoning_agent_chain(model_name, session_id)\n",
    "    # Retrieve 'history' from memory\n",
    "    memory_variables = memory.load_memory_variables({})\n",
    "    history = memory_variables.get(\"history\", \"\")\n",
    "    input_data = {\n",
    "        \"input\": user_input,\n",
    "        \"history\": history,\n",
    "    }\n",
    "    # Generate the response\n",
    "    response = chain.invoke(input_data)\n",
    "    # Update the memory with the new messages\n",
    "    memory.chat_memory.add_message(HumanMessage(content=user_input))\n",
    "    memory.chat_memory.add_message(AIMessage(content=response))\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cache\n",
    "cache = diskcache.Cache(\"./reasoning_agent_cache\")\n",
    "\n",
    "def cached_reasoning_agent(user_input, model_name, session_id):\n",
    "    cache_key = f\"{user_input}:{model_name}:{session_id}\"\n",
    "    if cache_key in cache:\n",
    "        print(\"Cache hit\")\n",
    "        return cache[cache_key]\n",
    "    else:\n",
    "        print(\"Cache miss\")\n",
    "        response = reasoning_agent(user_input, model_name, session_id)\n",
    "        cache[cache_key] = response\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4380/2025938851.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  session_memories[session_id] = ConversationBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: The **French national football team** won the FIFA World Cup in 2018. They defeated Croatia in the final match with a score of 4-2.  This was France's second World Cup title, their first since 1998. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Session ID for the user\n",
    "session_id = \"user1\"\n",
    "\n",
    "# Choose the model ('openai' or 'gemini')\n",
    "model_name = \"gemini\"\n",
    "\n",
    "# First input from the user\n",
    "user_input = \"Hello, who won the FIFA World Cup in 2018?\"\n",
    "\n",
    "# Get the response\n",
    "response = reasoning_agent(user_input, model_name, session_id)\n",
    "print(f\"AI: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Of course! The final match between France and Croatia was a thrilling affair. \n",
      "\n",
      "* **Early Goals:** France got off to a strong start, scoring two goals in the first half through Mario Mandzukic (own goal) and  Antoine Griezmann's penalty. \n",
      "* **Croatia Fights Back:** Croatia refused to give up and pulled one goal back through Ivan Perišić in the 28th minute.  \n",
      "* **Second Half Drama:** The second half saw more goals.  Kylian Mbappé scored a stunning goal in the 65th minute, extending France's lead. However, Croatia again fought back, with Perišić scoring his second goal in the 69th minute.\n",
      "* **Late Winner:** France ultimately sealed the victory with a fourth goal in the 18th minute of extra time, thanks to a powerful strike by  Paul Pogba. This goal proved to be the decider, and France lifted the World Cup trophy.\n",
      "\n",
      "It was a dramatic and exciting match, full of twists and turns.  Both teams played with passion and skill, making it a truly memorable final. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next input from the user\n",
    "user_input = \"Can you tell me more about the final match?\"\n",
    "\n",
    "# Get the response\n",
    "response = reasoning_agent(user_input, model_name, session_id)\n",
    "print(f\"AI: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation History:\n",
      "human: Hello, who won the FIFA World Cup in 2018?\n",
      "ai: The **French national football team** won the FIFA World Cup in 2018. They defeated Croatia in the final match with a score of 4-2.  This was France's second World Cup title, their first since 1998. \n",
      "\n",
      "human: Can you tell me more about the final match?\n",
      "ai: Of course! The final match between France and Croatia was a thrilling affair. \n",
      "\n",
      "* **Early Goals:** France got off to a strong start, scoring two goals in the first half through Mario Mandzukic (own goal) and  Antoine Griezmann's penalty. \n",
      "* **Croatia Fights Back:** Croatia refused to give up and pulled one goal back through Ivan Perišić in the 28th minute.  \n",
      "* **Second Half Drama:** The second half saw more goals.  Kylian Mbappé scored a stunning goal in the 65th minute, extending France's lead. However, Croatia again fought back, with Perišić scoring his second goal in the 69th minute.\n",
      "* **Late Winner:** France ultimately sealed the victory with a fourth goal in the 18th minute of extra time, thanks to a powerful strike by  Paul Pogba. This goal proved to be the decider, and France lifted the World Cup trophy.\n",
      "\n",
      "It was a dramatic and exciting match, full of twists and turns.  Both teams played with passion and skill, making it a truly memorable final. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "memory = get_memory(session_id)\n",
    "print(\"Conversation History:\")\n",
    "for message in memory.chat_memory.messages:\n",
    "    print(f\"{message.type}: {message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: The coach of the French national team that won the 2018 FIFA World Cup was **Didier Deschamps**. He is a former French professional footballer who captained the French team to victory in the 1998 World Cup.  Deschamps's tactical approach and leadership were instrumental in France's triumph in 2018. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# User asks a follow-up question\n",
    "user_input = \"Who was the coach of the winning team?\"\n",
    "\n",
    "# Get the response\n",
    "response = reasoning_agent(user_input, model_name, session_id)\n",
    "print(f\"AI: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an intelligent assistant specialized in answering questions and providing detailed explanations.\\n\"\n",
    "    \"Maintain the following conversation history:\\n\"\n",
    "    \"{history}\\n\\n\"\n",
    "    \"When responding, be clear, concise, and provide any relevant information that may assist the user.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727874400.999491    4380 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /home/codespace/.local/lib/python3.12/site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from streamlit) (4.25.5)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /home/codespace/.local/lib/python3.12/site-packages (from streamlit) (2.32.3)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Downloading rich-13.9.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from streamlit) (8.5.0)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /home/codespace/.local/lib/python3.12/site-packages (from streamlit) (3.1.43)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /home/codespace/.local/lib/python3.12/site-packages (from streamlit) (6.4.1)\n",
      "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/codespace/.local/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Collecting narwhals>=1.5.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Downloading streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.1-py3-none-any.whl (242 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading narwhals-1.9.0-py3-none-any.whl (178 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: watchdog, toml, pyarrow, narwhals, mdurl, blinker, pydeck, markdown-it-py, rich, altair, streamlit\n",
      "Successfully installed altair-5.4.1 blinker-1.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 narwhals-1.9.0 pyarrow-17.0.0 pydeck-0.9.1 rich-13.9.1 streamlit-1.39.0 toml-0.10.2 watchdog-5.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def reasoning_agent(user_input, model_name, session_id):\n",
    "    try:\n",
    "        chain, memory = create_reasoning_agent_chain(model_name, session_id)\n",
    "        # Rest of the code...\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        return \"I'm sorry, but I encountered an error while processing your request.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
